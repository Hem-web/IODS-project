---
title: "Untitled"
author: "Hem Raj Bhattarai"
date: "27.11.2019"
output: html_document
---

# chapter5: Dimensionality reduction techniques
## Reading the data

```{r setup22, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("~/IODS-project")
human <- read.table("~/IODS-project/data/human.txt")
str(human)
dim(human)
#Read the required packages
library(MASS)
library(dplyr)
library(ggplot2)
library(GGally)
```


## Summary and Distribution of the data

```{r setup23, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
summary(human)
ggpairs(human, title = "Distribution of Human Development Indices",
        lower=list(continuous=wrap("smooth", size = 0.5, colour="red")),
        diag=list(continuous=wrap("barDiag", fill="blue", bins = 40)))
```

1. Distribution and Relationship between variables:
  * Among all, expected years of schooling (***expec.edu***) is showing a homogenous distribution. However, secondary education ratio (***secedu.R***) and ***par.percent*** are also somewhat close to normal distribution. 
  * Life expectancy at birth (***life.expB***) and ***expec.edu*** showed a strongest positive relationship among all suggesting that people expected to live during birth recevive higher education. Following this maternal mortality rate (**MMR**) and adolescent birth rate (**adols.BR***), gross national income per capita (**GNI.capita**) and ***expec.edu***, and , ***life.expB*** and ***secedu.R*** possess the strongest correlation.
  * In contrast, **MMR** and ***life.expB*** showed a strongest negative correlation among all indication that greater the MMr higher the risk of losing life of new born.
  * There seems to be no relationship between ***secedu.R*** and ***labour.ratio*** at all.

## PCA on non-standarized data

```{r setup24, message=FALSE,  warning=FALSE}
human_pca<-prcomp(human)# perform principal component analysis (with the SVD method using prcomp)
biplot(human_pca, choices = 1:2, col = c("grey40", "deeppink2")) # draw a biplot of the principal component representation and the original variables
```

## PCA on standarized data: ***Way I***
```{r setup25, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
human_std <- scale(human)      # standardization  of human data set and save it under human_std
summary(human_std)             # print out summaries of the standardized variables
pca_human <- prcomp(human_std) # perform PCA (with the SVD method using prcomp)
s <- summary(pca_human)                           #summary of the pca_human data
pca_pr <- round(1*s$importance[2, ], digits = 5)  # proportion of variance captured by each PC
pca_pr <- round(100*s$importance[2, ], digits = 1)#Percentage (%) of variance captured by each PC's. By changing 1 to 100 in round command and 5 to 1 in digis command.
print(pca_pr)
pc_lab<-paste0(names(pca_pr), " (", pca_pr, "%)") #create object pc_lab to be used as axis labels

# draw a biplot. Label the x and y axis by giving thepc_lab's first value as pc_lab[1] ans did same for y axis.
biplot(pca_human, cex = c(0.8, 1),         
       col = c("grey40", "deeppink2"), 
       xlab = pc_lab[1], ylab = pc_lab[2])
```

2. Is there any difference between PCA's of non-standarized and standarized?
 * Yes, there is difference between these two PCA's. In biplot of PCA with non-standarized data we see only one arrow showing GNI per capita. While in biplot of PCA with standarized data we can see many arrows showing several HDI indices pointing towards two PC's.In order to minimize the emphasis of PCA on variables having higher variances (squared deviation from the mean is used) we must standarized the data before PCA. For e.g if there are two variables one with high values e.g in our case GNI which is in 1000's of dollar whereas other variables such as MMR, secedu.R and so on are relatively in smaller value. So, if we don not standardized such data the PCA will give high weight to the feature having larger variance e.g to GNI, as in seen in non-standarized biplot. Therefore, inorder to provied fair comparison between the explained variance in the dataset we need to standardize the data set. As a result we can possible see many variables linked to two main PC's.   

3. Personal interpretation on first two PC's of PCA from standarized data.
 * In the biplot of standarized data, pC1 explains the almost 54 % of varinace in the data set which is likely linked with the MMR and dolsescent birth rate. 
 * On the other hand,PC2 explains about 16 % of variance inthe data set which is then associated with ***expect.edu***, ***GNI.capita***, ***escedu.R***, and ***life.expB***.

## Biplot on standarized data set- ***Way II*** 
```{r setup26, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
pca_human <- prcomp(human_std, center = TRUE,scale. = TRUE)
summary(pca_human)             #Here we can see the PC1-8 with SD and proportion of variance.
library(devtools)
install_github("vqv/ggbiplot") #A ggplot2 based biplot. It provides a drop-in replacement for biplot.princomp().
library(ggbiplot)              #ggbiplot is only available for R3.6.1
ggbiplot(pca_human)+ ggtitle("PCA on HDI with labels as dots") # Here we can see that biplot contains arrows representing the variables and dots as conutries. So lets label the dots with countries name.
ggbiplot(pca_human, labels=rownames(human_std))+ ggtitle("PCA on HDI with labels as countries") #The labels with countries name is changed by giving command forlabes as rownames("data").
ggbiplot(pca_human, labels=rownames(human_std), obs.scale = 1, var.scale = 1,)+ ggtitle("PCA on HDI with changed scale on observation and variance") #In this we changed the scale of observation and variables and also added the title.
```

## Multiple correspondance analysis (MCA)
```{r setup27, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(FactoMineR)
library(factoextra)
library(tidyr)
data(tea)
dim(tea)
str(tea)
```

## Visulaization of the tea data
```{r setup28, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#Let' select all the categorical varibles in the tea data except variable age in column 19 which is numerical
tea1<-tea[, -19]
#Lets again split this data into two subsets.tea2 contains columns 1 to 15 and tea 3 from 16 to 30.
tea2<-tea1[, c(1:15)] 
tea3<-tea1[, c(16:30)] 

gather(tea2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + ggtitle("DATA on TEA") + geom_bar(fill="#E69F00",color="pink")+theme(plot.title = element_text(color = "darkred"))

gather(tea3) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + ggtitle("DATA on TEA") + geom_bar(fill="green",color="pink")+theme(plot.title = element_text(color = "red"))
```

## MCA of the tea data (full dataset)
```{r setup29, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
data(tea)
mca <- MCA(tea, quanti.sup = 19, quali.sup=20:36) #Do MCA using function MCA() on full data tea.
summary(mca)
```

4. **MCA result Interpretation**: 
 * First of all it is important to know that MCA is mainly used for categorical variables as we have in tea dataset.
 * However, continous variable can be used as a background variable.
 * The summary of mca showed decending order of variance % explained (as Eigenvalues) by each dimensions. For e.g Dim1 has explained close to 10 % of variance while Dim27 explained only 1% of variance.
 * Following that, summary result showed the contribution (**ctr**) of each indivial on producing dimensions. And we can say that Dim1 and Dim2 are mainly influenced by individual 6 and 10 respectively. 
 * On the other hand **cos2** represent the quality of individual on dimensions. If **cos2** is closer to 1 then we can say that individual is well projected to the dimensions and in our case individual 4 and 10 are well representing the Dim1 and Dim2 respectively.
 * MCA has shown the significance of active categorical variables with respect to zero as a V-test. If v-test is between -2 to 2 then categories as coordinate in not significantly different than zero, if v-test is >2 if categories is significantly greater than zero and is < -2 if categories is significantly less than zero. From this, we can say that for Dim1 breakfast ,tea time,evening, lunch, and Not.dinner categories are greater than zero. Whereas, same hold true with Not.breakfast, Not.lunch and dinner Dim2.
 * Following that, MCA has also given the influence of each categorical variable sin dimensions as **(eta2)**, which is similar of ANOVA test. The value close to 1 suggest that there is strong link between dimensions and categorical variables. So we can say that tearoom, tea.time and fiends are strongly linked with Dim1. In contrast there is no such strong relationship between the varaibles and Dim2. 
 * There are also result for supplementary categories similar to categorical categories However they do not contribute to the construction of dimensions.
 * Lastly,  MCA has give the continious variable of the dataset and its relationship with the dimensions.


## Exploring various MCA plotting options
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Here I reduced the font size by giving cex command.Added the factors that contributed the most by giving the comman selectMod. And the result, ther eno label bu only points.Changed the color of less important varibales to dark grey to that tey are more visible. For e.g , it is helpful to use such graph during PP presentation.

plot(mca, invisible=c("ind", "quali.sup"), cex=.8, selectMod = "contrib 20", unselect = "grey30") 
plot(mca, invisible=c("quali.sup"), cex=.8, selectMod = "cos2 0.1", select = "contrib 10") 
plot(mca, invisible=c("var", "quali.sup"), habillage = "frequency") #Different color for individual.

#Here Lets creat a new subset from the tea data, perform the MCA and plot the biplot showing cos2 values as group.
tea2<-tea1[, c(1:5)]
mca1 <- MCA(tea2, graph = FALSE)
fviz_mca_ind(mca1, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, # Avoid text overlapping (slow if many points)
             ggtheme = theme_minimal())

```

5. Comment on MCA plots:
 * In the first plot under this title, i have reduced the font sizes by adding cex command and also gave the command to plot the 20 most contributing individual to dimensions.Also i changed the color of less important variables to dark grey. 
 * In second plot, I plotted the quality of individuals (cos2 greater than 0.1) on dimensions only for 10 most contributing variables as shown by dark blue color.
 * In third biplot, different color is assigned for individual variables using habillage command. As a group I  used the color for frequency on drinking tea.
 
 * Similar to second biplot, fourth biplot represent the individuals by their cos2 values.

