---
title: "Chapter4"
author: "Hem Raj Bhattarai"
date: "19.11.2019"
output: html_document
---

#Chapter4: Clustering and Classification
 + In this chapter we are going to learn about methods that help us to identify the clusters (obersavtionsthat falls in one point or not homogenous) via clustering and classifying (giving names) and interpreting them based on their clusters structures or forms via classification method. 
+ ***K-means clustering*** is most typical technique used to identify clustered observations in the data. Also ***hierarchical clustering*** methods is quite popular, giving tree-like dendrograms as their main output.
+ Presence or absence of clusters in the data is easy to identify but knowing more e.g how many, giving names etc is not that easy.
+ Knowing more about clusters after identifying them is done by using different form of analysis e.g. ***discriminant analysis*** which operates with the (now) known clusters, asking: "what makes the difference(s) between these groups (clusters)?"
+ Additionally, ***distance*** (or dissimilarity or similarity) approach on classifying clusters will be also discussed.
updateR()

## Reading the data
```{r echo=FALSE, message=FALSE, warning=FALSE}
setwd("~/IODS-project")
library(ggplot2)
library(GGally)
library(MASS)
data(Boston)
summary(Boston)
dim(Boston)
str(Boston)
D1<-dplyr::select(Boston, crim, zn,indus,chas,nox,rm,age) #Selecting half of the data and making a subset.
D2<-dplyr::select(Boston, dis,rad,tax,ptratio, black,lstat,medv)

ggpairs(D1, title = "Distribution1",
    lower = list(combo =wrap("facethist", bins = 20)))

ggpairs(D2, title = "Distribution2",
    lower = list(combo =wrap("facethist", bins = 20))) 

```

+ Distribution of the variables:Looking in the both distribution figures all the variables are not homogenously distributed except ***rm** (average no of room per dewelling).
+ Relationship between the variables:In the data ***rad***(accessibility to radial highways) and ***tax*** shows a strong positive association as seen in the "Distribution2". On the other hand in the same figure a similar but negative association is also seen between ***Istat*** and ***medv***.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(corrplot)
cor_matrix<-cor(Boston) 
corrplot(cor_matrix, method="circle", order = "hclust", addrect = 2, col = terrain.colors(100), cl.ratio = 0.2, cl.align = "r")
```

## Standarization of the data
 + During standarization of the data we scale the variables so that their mean changes to zero and standard deviation is converted to 1.
```{r echo=FALSE, message=FALSE, warning=FALSE}
boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled<-as.data.frame(boston_scaled)
bins <- quantile(boston_scaled$crim)
print(bins)
crime <- cut(boston_scaled$crim, breaks = bins,labels=c("low","med_low","med_high", "high"), include.lowest = TRUE)
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
dim(boston_scaled)
str(boston_scaled)
```
 + After scaling the variables, the mean value of each has now changed to zero. 
 + During standarization of data, we also created a categorical variable ***crime*** and uses the quantiles as break points and saved the scaled data as data frame.
 + Following this we also divided the dataset into ***train dataset*** and ***test dataset*** so that 80 % o data belongs to first dataset. In ***test dataset*** variable ***crime*** is excluded just becuase that this variable will be used to define the classes that will help to generalize our results from train dataset.
```{r echo=FALSE, message=FALSE, warning=FALSE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- dplyr::select(test, crime)
test <- dplyr::select(test, -crime)
str(train)
str(test)
```

## LDA

```{r echo=FALSE, message=FALSE, warning=FALSE}
lda.fit <- lda(crime ~ ., data = train)                 # linear discriminant analysis
print(lda.fit)                                          # print the LDA fit  
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes) # ploting the LDA fit to have a firstlook
lda.pred <- predict(lda.fit, newdata = test)          # predict classes with test data
lda.pred$class
```

## Distance measurement
```{r}
library(MASS)
data('Boston')
boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled<-as.data.frame(boston_scaled)
dist_eu <- dist(boston_scaled)
summary(dist_eu)
boston_scaled<-as.matrix(boston_scaled, method= "manhattan") # saving the scaled data under manhattan matrix.
dist_man <- dist(boston_scaled)
summary(dist_man)
```

## K-means clustering
```{r}
# k-means clustering
km <-kmeans(Boston, centers = 4)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
pairs(Boston[6:10], col = km$cluster)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <-kmeans(Boston, centers = 2)
pairs(Boston, col = km$cluster)
```
+ A abrupt drop in  the total within-cluster sum of squares (WCSS) happneded when the number of clusters (x-axis) reached around 2 suggesting that optimum no of clusters is most likely 2 or less. No change in WCSS even after increasing the number of clusters upt 10 also suggest result produced using 2 or less number of cluster could be the similar.

##BOnus
```{r echo=FALSE, message=FALSE, warning=FALSE}
data(Boston) # load the Boston data set
boston_scaled1<- scale(Boston)# scale the Boston data set again - named boston_scaled1
km1<-kmeans(boston_scaled1, centers = 3)# k-means clustering
cluster <- km1$cluster
boston_scaled1<- data.frame(boston_scaled1,cluster) # add the cluster number to the df
lda.fit_cluster <- lda(cluster ~ ., data = boston_scaled1)#linear discriminant analysis of clusters vs. all other variables
lda.fit_cluster# print the lda.fit object

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes1 <- as.numeric(boston_scaled1$cluster)# target classes as numeric

plot(lda.fit_cluster, dimen = 2, col = classes1, pch = classes1, main = "LDA biplot using three clusters 1, 2 and 3")# plot the lda results
lda.arrows(lda.fit_cluster, myscale = 2)
```


##Super Bonus
```{r echo=FALSE}
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

#Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.
library(plotly)
pal <- c("red", "blue")
plot1<-plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot1


```


